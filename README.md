# SimulatorAttackplus
This repo is based on CVPR 2021 Ma's Simulator Attack. The reference is [here](https://github.com/machanic/SimulatorAttack).

# Motivations
## FABM
Through feature extraction and visualization of the simulator model in the initial state and the model selected as the black-box target, we find that the feature attentional area of a image is almost the same between the two models.

The feature layer and attentional region visualization of first 9 images from the first batch input into initial simulator model and black-box model. The first line is the original images. The second line is the feature layer visualization of initial meta simulator model. The third line is the feature attentional region of initial meta simulator model. The forth line is the feature layer visualization of black-box target model (PyramidNet272). The last line is the feature attentional region of black-box model.

After comparing the feature attentional regions between simulator model and black-box target model, we find the attentional areas of both models almost overlap with each other. So we conclude that some of the feature layer information in meta simulator model can be used in black-box attack too due to their similarity.

<img src="/features.png" width="45%">

## UCM and LSSIM
The picture below indicates that the classification ability varies relatively in initial simulator model and black-box target model. And this classification ability is the key point that simulator model need to learn during finetuning process after.
<img src="/classification.png" width="110%">

# Parameters
The detail information show in the default parameters setting of Simulator Attack+ Table I.
<img src="/params.png" width="60%">

# Ablation Study
## Ablation Study for Feature Attentional Boosting Module
We firstly compare our two mentioned methods in FABM and decide use the momentum boosting module as the finally version of our Simulator Attack+. Then we conduct a group of experiments for our opinion by only adding FABM and adjusting the weight values of current direction and average direction in it. The range of current direction weight value is set from 0.9 to 1. Table II shows results. Each round of attacking takes 3 days for average. And these experiments are untargeted attack under l1 norm on CIFAR-100 dataset.
<img src="/ablation_study1.png" width="70%">

## Ablation Study for Linear Self-adaptive Simulatorpredict Interval Mechanism
Table III clearly shows the results of experiments about Simulator Attack added with different parameters of LSSIM module only. This module can reach considerable positive influence under proper setting parameter pairs when conducting targeted attack, as such attack has to use large amount of queries.

<img src="/ablation_study2.png" width="60%">

## Ablation Study for Unsupervised Clustering Module
We conduct targeted attack within l2 norm on CIFAR-10 to test the enhance effect of our unsupervised clustering module(UCM). Table IV shows the results compared with the baseline. As the time that this attack cost is very long, we only choose one round to show the improvement.
<img src="/ablation_study3.png" width="70%">

# Requirement
Pytorch 1.4.0 or above, torchvision 1.3.0 or above, bidict, pretrainedmodels 0.7.4, opencv-python

# Folder structure
```
+-- configures
|   |-- meta_simulator_attack_conf.json  # the hyperparameters setting of simulator attack
|   |-- Bandits.json  # the hyperparameters setting of Bandits attack
|   |-- prior_RGF_attack_conf.json  # the hyperparameters setting of RGF and P-RGF attack
|   |-- meta_attack_conf.json  # the hyperparameters setting of Meta Attack
|   |-- NES_attack_conf.json  # the hyperparameters setting of NES Attack
|   |-- SWITCH_attack_conf.json  # the hyperparameters setting of SWITCH Attack
+-- dataset
|   |-- standard_model.py  # the wrapper of standard classification networks, and it converts the input image's pixels to the range of 0 to 1 before feeding.
|   |-- defensive_model.py # the wrapper of defensive networks, and it converts the input image's pixels to the range of 0 to 1 before feeding.
|   |-- dataset_loader_maker.py  # it returns the data loader class that includes 1000 attacks images for the experiments.
|   |-- npz_dataset.py  # it is the dataset class that includes 1000 attacks images for the experiments.
|   |-- meta_two_queries_dataset.py  # it is the dataset class that trains the Simulator.
|   |-- meta_img_grad_dataset.py  # it is the dataset class that trains the auto-encoder meta-learner of Meta Attack.
+-- meta_simulator_bandits
|   +-- learning
|       +-- script
|           |-- generate_bandits_training_data_script.py   # it can generate the training data that is generated by using Bandits to attack multiple pre-trained networks.
|       |-- train.py  # the main class for training the Simulator.
|       |-- meta_network.py  # the wrapper class of the meta network, and it can transform any classification network to the meta network in meta-learning.
|       |-- meta_distillation_learner.py  # it includes the main procedure of meta-learning.
|       |-- inner_loop.py  # it includes the inner update of meta-learning.
|   +-- attack
|       |-- meta_model_finetune.py  # it includes the class used for fine-tuning the Simulator in the attack.
|       |-- simulate_bandits_attack_shrink.py  # it includes the main procedure of Simulator Attack.
+-- cifar_models   # this folder includes the target models of CIFAR-10, i.e., PyramidNet-272, GDAS, WRN-28, and WRN-40 networks.
+-- tiny_imagenet_models   # this folder includes the target models of TinyImageNet, e.g., DenseNet and ResNeXT
+-- xxx_attack  # other attacks for the compared experiments in the paper.
|-- config.py   # the main configuration of Simulator Attack, remember to modify PY_ROOT to be the project's folder path in your machine environment.
```
# How to attack
Option 1: attack PN272, GDAS, WRN-28, and WRN-40 one by one with one command line untargeted.

`python simulate_bandits_attack_shrink1_for_untargeted.py --gpu 0 --norm linf --epsilon 0.031372 --batch-size 100 --dataset CIFAR-10 --data_loss cw
 --distillation_loss mse --meta_arch resnet34 --test_archs`

Option 2: attack PN272, GDAS, WRN-28, and WRN-40 one by one with one command line targeted

`python simulate_bandits_attack_shrink_kmeans_for_targeted.py --gpu 0 --norm l2 --epsilon 4.6 --batch-size 100 --dataset CIFAR-100 --data_loss cw --distillation_loss mse --meta_arch resnet34 --test_archs --targeted`

Option 3: attack defensive model of adversarially trained ResNet-50:

`python simulate_bandits_attack_shrink1_for_untargeted.py --gpu 0 --norm linf --dataset CIFAR-10 --data_loss cw --distillation_loss mse --meta_arch resnet34 --arch resnet50 --attack_defense --defense_model adv_train --batch-size 100`

Folder named `logs` will be generated when attack begins. The log and experimental result file (a `.json` file that includes all queries and the success rate) will be saved to the `logs` folder.`
